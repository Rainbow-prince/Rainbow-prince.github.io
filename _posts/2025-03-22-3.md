---
layout: post
title: 【梳理】线性回归模型 和 优化方法
subtitle: “概念的理清”
author: Rainbow-Prince
header-mask: 0.5
header-img: img/Preface__Computer Vision： Algorithms and Applications.png
tags:
  - ML/DL
---

### 线性回归

线性回归是一种模型，所谓”线性“二字，既不代表要拟合的函数是线性的，也不代表每个”自变量“是线性的。这么说可能让人更疑惑，我们可能非常熟悉于

$$
 y = kx_1 + b
$$

或者

$$
 y = a_0 + a_1x_1 + a_2x_2
$$
或者更应该写成

$$
 y = a_0 + a_1x_1 + a_2x_2 + \dots + a_n x_n
$$

但是，其实这些$x_n$这些”自变量“，不一定得是”自变量“，可以叫成”基函数“；也不一定得是”线性“的，这和它叫”线性回归“并不矛盾，因为它是这些”基函数“的线性组合。

所以我们大可以写成，比如
$$
y = a_0 + a_1x + a_2x^2 + \ldots + a_nx^n
$$
或者其它更多奇奇怪怪的函数的组合，**用这些非线性的函数，经过线性组合，拟合某些函数。**


### 优化方法

#### 最小二乘法

#### 梯度下降法